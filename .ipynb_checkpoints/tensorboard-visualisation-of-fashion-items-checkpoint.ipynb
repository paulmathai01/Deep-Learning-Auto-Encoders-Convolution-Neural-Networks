{"cells":[{"metadata":{"_uuid":"3e3f83ba0210bb44bdff7a34b686038c20a22d7c"},"cell_type":"markdown","source":"## If you like this kernel greatly Appreciate an UPVOTE. \n\n# Tensorboard Visualisation of Fashion Items!!!\nThis Kernel is about Data Visualisation of high Â  dimensional data to a lower dimension using Tensorboard with t-SNE and PCA dimensionality reduction techniques and exploration of data points with multiple parameter tuning . \n\n### What is an Embedding?\n\n- Embedding is a way to map discrete objects (images, words, etc.) to high dimensional vectors.\n\n- The individual dimensions in these vectors typically have no inherent meaning. Instead, it's the overall patterns of  location and distance between vectors that machine learning takes advantage of.\n\n\n- Embeddings are important for input to machine learning. Classifiers, and neural networks more generally, work on vectors of real numbers. They train best on dense vectors, where all values contribute to define an object. \n\n- Embeddings map objects to vectors, applications can use similarity in vector space (for instance, Euclidean distance or the angle between vectors) as a robust and flexible measure of object similarity. One common use is to find nearest neighbors.\n\n### Workflow  \n\n#### To do a basic embedding visualisation we need to follow the below workflow steps \n\n-  Read the Fashion MNIST data and create an X (image) and Y (label) batch\n\n-  Create a Summary Writer\n\n-  Configure the projector\n\n-  Create the embedding Tensor from X\n\n-  Run the TF session and create a model check-point\n\n-  Create the sprite image\n\n-  Create the metadata (labels) file"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"#Import Libraries\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import Image\nfrom IPython.core.display import HTML \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"- Read the Fashion MNIST test data and create an X (image) and Y (label) batch\n\nFirst we will load the fashion mnist test data into a pandas dataframe and subsequently convert into a numpy array with datatype as float32"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7542ad24fed31af12456f251fcd39da0ebc3f140","_kg_hide-input":true},"cell_type":"code","source":"test_data = np.array(pd.read_csv(r'/kaggle/input/fashion-mnist_test.csv'), dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b714ef3cc6d07bdcc42474b495ba305383d49554"},"cell_type":"markdown","source":"Lets consider around 2500 images as part of the embedding"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"02d0bcd065af5375961b67b6f324682ec0a202d4","_kg_hide-input":true},"cell_type":"code","source":"embed_count = 2500","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"493fe6070feb83f51e5ff55abe0457fb0e0daec8"},"cell_type":"markdown","source":"Now we will split our data into x_test for storing images and y_test for storing labels. Since all images are ranging from 0-255 we will need to rescale all of them by dividing with 255 so that it reflects between 0 and 1."},{"metadata":{"trusted":true,"_uuid":"8a025690bdeaed86ac258fd8209f039cc418d65d","collapsed":true},"cell_type":"code","source":"x_test = test_data[:embed_count, 1:] / 255\n\ny_test = test_data[:embed_count, 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"522d1e6bd0504f94ece1e316dbf02f6c3c23b64c"},"cell_type":"markdown","source":"To store all the events in a log directory for tensorboard view we need to mention the path as below"},{"metadata":{"_uuid":"5ab7f32e59e6d846c7266a8cfaaef1b7b3c91b00"},"cell_type":"markdown","source":"- ##### Create a Summary Writer"},{"metadata":{"_uuid":"9f306511e0425ecba258ca8a92c8b8056aa550bb"},"cell_type":"markdown","source":"summary_writer = tf.summary.FileWriter(r'C:\\FashionMNIST\\logs')"},{"metadata":{"_uuid":"2e5bbc57c3b35d61579da2b2a10f08137ec2611e"},"cell_type":"markdown","source":"#### Creating the embedding variable with all the images defined above under x_test \n\nembedding_var = tf.Variable(x_test, name='fmnist_embedding')"},{"metadata":{"_uuid":"2db20175aeee705eb597f7f1d9bb4def7de321b8"},"cell_type":"markdown","source":"- ##### Configure the projector\nThis is the important part of embedding visualisation. Here we specify what variable we want for the project, what the metadata path is (the names and classes), and where to save the sprites."},{"metadata":{"_uuid":"b40846777fc5c9f2a764cdda1faf8b7270c1f0a9"},"cell_type":"markdown","source":"config = projector.ProjectorConfig()\nembedding = config.embeddings.add()\nembedding.tensor_name = embedding_var.name\nembedding.metadata_path = os.path.join(logdir, 'metadata.tsv')\nembedding.sprite.image_path = os.path.join(logdir, 'sprite.png')\nembedding.sprite.single_image_dim.extend([28, 28])"},{"metadata":{"_uuid":"98e4cd84185f851d577b0ba5b31869a0ca7dfb6c"},"cell_type":"markdown","source":"- ##### Create the embedding Tensor from X"},{"metadata":{"_uuid":"d097469edb573ac2f67b25bad4a07b02bf96d613"},"cell_type":"markdown","source":"projector.visualize_embeddings(summary_writer,config)"},{"metadata":{"_uuid":"f073d01db1feb8b89d0f6349224f92bf6bcced83"},"cell_type":"markdown","source":"- ##### Run the TF session and create a model check-point"},{"metadata":{"_uuid":"fd9e89dba37730b7f28e1f59cacfc4aded653b04"},"cell_type":"markdown","source":"with tf.Session() as sesh:\n   sesh.run(tf.global_variables_initializer())\n   saver = tf.train.Saver()\n   saver.save(sesh, os.path.join(logdir, 'model.ckpt'))"},{"metadata":{"_uuid":"ab8b2710c64b806c021af955c3dd78b7554bb423"},"cell_type":"markdown","source":"- ##### Create the sprite image"},{"metadata":{"_uuid":"ed22deae33f9532c673813d0f3c0fd4d11ffd5a9"},"cell_type":"markdown","source":"rows = 28\ncols = 28\nlabel = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n          'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nsprite_dim = int(np.sqrt(x_test.shape[0]))\nsprite_image = np.ones((cols * sprite_dim, rows * sprite_dim))\nindex = 0\nlabels = []\nfor i in range(sprite_dim):\n    for j in range(sprite_dim):\n        \n        labels.append(label[int(y_test[index])])\n        \n        sprite_image[\n            i * cols: (i + 1) * cols,\n            j * rows: (j + 1) * rows\n        ] = x_test[index].reshape(28, 28) * -1 + 1\n        \n        index += 1"},{"metadata":{"_uuid":"4984bd968392f3f77f68c8afdaf3723a143cff4d"},"cell_type":"markdown","source":"- ##### Create the metadata (labels) file"},{"metadata":{"_uuid":"8d07d457b1b84a305cc8604a5324d79ab6b83f06"},"cell_type":"markdown","source":"with open(embedding.metadata_path, 'w') as meta:\n\n    meta.write('Index\\tLabel\\n')\n    for index, label in enumerate(labels):\n        meta.write('{}\\t{}\\n'.format(index, label))\n        \nplt.imsave(embedding.sprite.image_path, sprite_image, cmap='gray')\n\nplt.imshow(sprite_image, cmap='gray')\n\nplt.show()"},{"metadata":{"_uuid":"9392a432bd468c69f7c305677f5f389be2cb1b5a"},"cell_type":"markdown","source":"The above figure represents the grayscale fashion products representation in lower dimension.Let us look more closer by zooming the picture as below\n\n![](https://i.imgur.com/SF8KeUI.png)\n\n\n## How to run\n\nWe saved our MNIST fashion images, time to visualise it! Follow the steps below\n\n- Step 1 : Go to command prompt and type the following \nC:\\Users\\Pavan>tensorboard --logdir=C:\\FashionMNIST\\logs\n\n- Step 2: Now open a browser and navigate to http://127.0.0.1:6006 (note: this can change depending on your computer setup). \nYou should see this after navigating to the Projector Tab as shown below\n\n\n## Visualizing Embeddings\nVisualising embeddings is a powerful technique! It helps you understand what your algorithm learned, and if this is what you expected it to learn. \n\nTensorBoard includes the Embedding Projector, a tool that lets you interactively visualize embeddings. This tool can read embeddings from your model and render them in two or three dimensions.\n\nThe Embedding Projector has three panels:\n- Data panel on the top left, where you can choose the run, the embedding variable and data columns to color and label points by.\n- Projections panel on the bottom left, where you can choose the type of projection.\n- Inspector panel on the right side, where you can search for particular points and see a list of nearest neighbors.\n\n### Projections\nA projection is a method for taking these high dimensional vections and project them into a lower dimensional space.\n\nThe Embedding Projector provides three ways to reduce the dimensionality of a data set.\n\n* #### PCA ( Principal Component Analysis) \n\n- A linear deterministic algorithm (principal component analysis) that tries to capture as much of the data variability in as few dimensions as possible. PCA tends to highlight large-scale structure in the data, but can distort local neighborhoods. The Embedding Projector computes the top 10 principal components, from which you can choose two or three to view.\n\n![](https://i.imgur.com/sogsKUs.gif)\n\n\n\n\n"},{"metadata":{"_uuid":"62462204f576f92221cbcb5a3f29d01c8aeb9224"},"cell_type":"markdown","source":"* #### t-SNE (T-distributed stochastic neighbor embedding)  \n\n- A nonlinear nondeterministic algorithm (T-distributed stochastic neighbor embedding) that tries to preserve local neighborhoods in the data, often at the expense of distorting global structure. You can choose whether to compute two- or three-dimensional projections.\n\n- The algorithm is non-linear and adapts to the underlying data, performing different transformations on different regions.\n\n- A second feature of t-SNE is a tuneable parameter, âperplexity,â which says (loosely) how to balance attention between local and global aspects of your data. The parameter is, in a sense, a guess about the number of close neighbors each point has. The perplexity value has a complex effect on the resulting pictures.Its typically values are between 5 and 50.\n\n- Getting the most from t-SNE may mean analyzing multiple plots with different perplexities.\n\n- For the algorithm to operate properly, the perplexity really should be smaller than the number of points. Implementations can give unexpected behavior otherwise.\n\n- Epsilon is another parameter used to measure the learning rate \n\n- The t-SNE algorithm  naturally expands dense clusters, and contracts sparse ones, evening out cluster sizes. To be clear, this is a different effect than the run-of-the-mill fact that any dimensionality reduction technique will distort distances. Rather, density equalization happens by design and is a predictable feature of t-SNE.\n\n- Distances between well-separated clusters in a t-SNE plot may mean nothing.\n\n- Low perplexity levels often show cloud of points generated randomly which has no statistically interesting clusters (clumps).Recognizing these clumps as random noise is an important part of reading t-SNE plots.\n\n-  Topological information off a t-SNE plot can be read with views at multiple perplexities. \n\nSource: https://distill.pub/2016/misread-tsne/\n\n![](https://i.imgur.com/nSTCqrj.png)"},{"metadata":{"_uuid":"a89451ae1fbe1af59a829e298c4b59c2f5b10dd5"},"cell_type":"markdown","source":"* #### Custom: \n\n- A linear projection onto horizontal and vertical axes that you specify using labels in the data. You define the horizontal axis, for instance, by giving text patterns for \"Left\" and \"Right\". The Embedding Projector finds all points whose label matches the \"Left\" pattern and computes the centroid of that set; similarly for \"Right\". The line passing through these two centroids defines the horizontal axis. The vertical axis is likewise computed from the centroids for points matching the \"Up\" and \"Down\" text patterns.\n\n![](https://i.imgur.com/xrAJGAJ.png)\n\n"},{"metadata":{"_uuid":"e9f0e5cbf9cd0298938bccadea4bae3f53de2cf7"},"cell_type":"markdown","source":"#### Exploration\n\nYou can explore visually by zooming, rotating, and panning using natural click-and-drag gestures. Hovering your mouse over a point will show any metadata for that point. You can also inspect nearest-neighbor subsets. Clicking on a point causes the right pane to list the nearest neighbors, along with distances to the current point. The nearest-neighbor points are also highlighted in the projection.\nIt is sometimes useful to restrict the view to a subset of points and perform projections only on those points. To do so, you can select points in multiple ways:\n- After clicking on a point, its nearest neighbors are also selected.\n- After a search, the points matching the query are selected.\n- Enabling selection, clicking on a point and dragging defines a selection sphere."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b002ab99335c3061bf4214d0b3d6f727dd6f56f0"},"cell_type":"markdown","source":"# If you like this kernel greatly appreciate an UPVOTE. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}